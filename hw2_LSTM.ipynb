{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFNN den farklı olan şeyler şunlardır:\n",
    "\n",
    "Adam: Öğrenme oranını otomatik ayarlayabilen popüler bir optimizasyon algoritması.\n",
    "\n",
    "EarlyStopping, ReduceLROnPlateau: Eğitim sürecini daha verimli hale getirmek için kullanılan callback’ler. Overfitting engellemek, fazla epoch çalıştırmamak amacıyla durdurma ya da öğrenme hızını (learning rate) düşürme yapar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "timesteps = 2000\n",
    "time = np.linspace(0, 1, timesteps)\n",
    "\n",
    "SoC = 100 * (1 - np.exp(-5 * time)) + np.random.normal(0, 2, timesteps)\n",
    "SoC = np.clip(SoC, 0, 100)\n",
    "\n",
    "SoH = 100 * np.exp(-0.002 * time * timesteps) + np.random.normal(0, 0.5, timesteps)\n",
    "SoH = np.clip(SoH, 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFNN den farklı olarak np.clip(SoC, 0, 100) kullandık. Bu kod değerlerin 0 ile 100 arasında kalmasını sağlıyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voltage = np.random.uniform(10, 14, timesteps)\n",
    "current = np.random.uniform(-50, 50, timesteps)\n",
    "temperature = np.random.uniform(15, 45, timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-14 arasında Volt değerleri, -50-50 arasında akım değerleri ve 15-45 derece arasında sıcaklık değerleri oluşturur.\n",
    "\n",
    "LSTM örneğinde daha gerçekçi bir senaryo için bataryayı etkileyebilecek voltaj, akım ve sıcaklık gibi ek parametreler de modele dahil edilmiş. Böylece LSTM zaman serisi verisindeki SoC, SoH dalgalanmalarının yanı sıra bu sensör verilerini de kullanarak daha iyi bir tahmin yapabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.column_stack((SoC, SoH, voltage, current, temperature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neden tek bir 2D dizi oluşturuyoruz?\n",
    "\n",
    "Modelin girdi olarak kullanacağı bütün değişkenleri (SoC, SoH, voltage, current, temperature) aynı tabloda (matriste) birleştiriyoruz.\n",
    "\n",
    "Bu şekilde, her satır bir zaman adımındaki tüm sensör bilgilerini (ve hedef değişkenleri) içeriyor.\n",
    "\n",
    "Daha sonra bu tabloyu LSTM/NN gibi modellere beslerken tek bir veri yapısı üzerinden işlem yapmak kolaylaşır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burdaki kodun FFNN den bir farkı yok. Sadece veriye ekstra sensör verileri eklmeiş olduk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])     \n",
    "        y.append(data[i + seq_length, :2])   \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(data_scaled, seq_length)\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM zaman serisi verisiyle (ardışık verilerle) çalışır.\n",
    "\n",
    "seq_length = 30 demek, model her seferinde 30 zaman adımını input olarak alır, 31. zamanın SoC ve SoH’sini tahmin etmeye çalışır.\n",
    "\n",
    "X boyutu (örnek_sayısı, 30, 5) ⇒ 30 zaman adımı, her adımda 5 özelliğimiz var (SoC, SoH, voltage, current, temperature).\n",
    "\n",
    "y boyutu (örnek_sayısı, 2) ⇒ Sadece SoC ve SoH değerlerini tahmin ediyoruz.\n",
    "\n",
    "Ayno şekilde burada da verinin %80 i eğitim %20 si tes için kullanılıyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(seq_length, X.shape[2])),\n",
    "\n",
    "    LSTM(64, return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    LSTM(32, return_sequences=False),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input(shape=(seq_length, X.shape[2]))\n",
    "\n",
    "X.shape bir tuple (demet) döndürür, örneğin (num_samples, seq_length, num_features).\n",
    "\n",
    "X.shape[0] → örnek sayısı (num_samples),\n",
    "\n",
    "X.shape[1] → zamansal uzunluk (seq_length),\n",
    "\n",
    "X.shape[2] → her zaman adımında kaç özelliğiniz (feature) olduğu (num_features).\n",
    "\n",
    "Burada seq_length=30 zaman adımı (örneğin son 30 ölçüm) ve X.shape[2]=5 özellik (SoC, SoH, voltage, current, temperature) var.\n",
    "\n",
    "Yani model, her bir örnek için 30×5 boyutunda bir zaman serisi alacak.\n",
    "----------------------------------------------------------------------------\n",
    "LSTM(64, return_sequences=True):\n",
    "\n",
    "İlk LSTM katmanı, 64 nöron.\n",
    "\n",
    "return_sequences=True diyerek, bir sonraki LSTM katmanına tüm zaman adımlarının çıktılarını aktarıyoruz.\n",
    "------------------------------------------------------------------\n",
    "LSTM(32, return_sequences=False)\n",
    "\n",
    "32: Bu LSTM katmanında 32 adet nöron (LSTM hücresi) vardır. Daha yüksek değer, daha fazla parametre ve daha yüksek öğrenme kapasitesi anlamına gelir; ancak eğitim süresini de uzatabilir.\n",
    "---------------------\n",
    "return_sequences=False:\n",
    "\n",
    "LSTM katmanları, varsayılan olarak her zaman adımındaki çıktıyı döndürebilir (True) veya sadece son zaman adımının çıktısını döndürebilir (False).\n",
    "\n",
    "Burada False olduğu için, katmandan sadece son zaman adımında elde edilen çıktı döner. Bu genellikle zaman serisi tahmini (regresyon) veya sınıflandırma gibi problemlerde, “en son adımın özet bilgisini” almak istediğimizde tercih edilir.\n",
    "\n",
    "Bir önceki LSTM katmanında return_sequences=True kullanılmışsa, o katmandan her bir zaman adımına ait çıktı alınır. Bu ikinci LSTM katmanı ise, o çıktıları (zaman serisi) teker teker okuyarak sonunda tek bir vektör üretir (son gizli durum – final hidden state). Bu vektör, zaman serisinin bütün bilgisini içinde barındırdığı varsayılır.\n",
    "----------------------------------------\n",
    "\n",
    "BatchNormalization()\n",
    "\n",
    "LSTM katmanından çıkan veriyi, her mini-batch’te ortalama ve standart sapma açısından yeniden ölçekler (normalleştirir).\n",
    "\n",
    "Amaç, eğitimi hızlandırmak ve dağılımı daha kararlı hale getirmektir. Özellikle LSTM gibi derin yapılarda, katmanlar arası dağılımların sürekli değişmesi “iç dağılım kayması (internal covariate shift)” yaratabilir; batch normalization bu etkiyi hafifletir.\n",
    "\n",
    "Daha istikrarlı gradyanlar sayesinde model daha hızlı yakınsar, öğrenme oranına daha az hassas hale gelir.\n",
    "----------------------------------------------\n",
    "Tam bağlı katman nedir ?\n",
    "\n",
    "Tam bağlı katman (Fully Connected Layer veya Dense Layer), önceki katmandaki her nöronun, bu katmandaki her nörona ayrı bir ağırlık (weight) ve bias ile bağlanması anlamına gelir. Yani girişteki her birim, katman içindeki tüm nöronlara etki eder. Bu sayede katmanda çok sayıda parametre bulunur ve ağ, giriş sinyalindeki her özelliği öğrenip birleştirebilecek esnekliğe sahip olur. \n",
    "\n",
    "-----\n",
    "Dense(16, activation='relu')\n",
    "\n",
    "LSTM katman(lar)ından gelen son çıktıyı (yani zaman serisinin işlenmiş hâlini) 16 nöron içeren “tam bağlı (fully connected)” bir katmana veriyoruz.\n",
    "\n",
    "Burada ReLU (Rectified Linear Unit) aktivasyon fonksiyonu kullanmak, doğrusal olmayan bir dönüşüm sağlayarak modelin öğrenebileceği temsil gücünü artırır.\n",
    "\n",
    "Amaç, LSTM’den gelen bilgiyi “ek bir katmanla” işleyerek, SoC ve SoH tahmini için daha rafine bir özellik (feature) çıkarımı yapmaktır.\n",
    "\n",
    "---------------------------------------------------------\n",
    "Dense(2, activation='sigmoid')\n",
    "\n",
    "Modelin çıkış katmanı: 2 nöron içerir, çünkü iki adet değer (SoC ve SoH) tahmin etmek istiyoruz.\n",
    "\n",
    "activation='sigmoid': Her nöron, çıktıyı 0 ile 1 arasına sıkıştırır.\n",
    "\n",
    "LSTM kodumuzda veriyi 0-1 aralığına ölçeklemiştik (MinMaxScaler). Dolayısıyla “modelin çıktısı” da 0-1 arasında olsun istiyoruz.\n",
    "\n",
    "Tahmin sonrasında “inverse_transform” uygulayarak bu değerleri tekrar 0–100 yüzdelik aralığına dönüştürüp SoC ve SoH olarak yorumlayabiliyoruz.\n",
    "---------------------------------------------------------\n",
    "Kısacası, son iki Dense katmanı:\n",
    "\n",
    "Önce 16 nöronla LSTM çıktılarını daha ayrıntılı işliyor (ReLU aktivasyonla),\n",
    "\n",
    "Ardından 2 nöronla (sigmoid) SoC ve SoH’yi 0-1 aralığında nihai olarak tahmin ediyor.\n",
    "-----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=tf.keras.losses.Huber())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFNN den farklı olarak burada Huber kullandık. Huber Loss: MSE ve MAE arasında bir denge sağlar, uç değerlerden (outlier) çok fazla etkilenmez, stabil bir öğrenme sunar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReduceLROnPlateau:\n",
    "\n",
    "monitor='val_loss' ⇒ validasyon kaybı (val_loss) gelişmiyorsa learning rate’i factor=0.5 oranında yarıya indirir.\n",
    "\n",
    "patience=3 ⇒ 3 epoch boyunca iyileşme olmazsa bu işlemi yapar.\n",
    "--------------------------------\n",
    "EarlyStopping:\n",
    "\n",
    "monitor='val_loss', patience=6 ⇒ 6 epoch boyunca val_loss iyileşmiyorsa eğitim durur.\n",
    "\n",
    "restore_best_weights=True ⇒ eğitim boyunca en iyi doğrulama skoruna (val_loss en düşük) sahip olan ağırlıkları al.\n",
    "---------------------------------\n",
    "epochs=30: Model veriyi 30 kez tarayacak veya early stopping devreye girerse daha önce duracak.\n",
    "---------------------------------------\n",
    "batch_size=32: Her adımda 32 örnekle güncelleme yapar (mini-batch).\n",
    "---------------------------------------\n",
    "validation_data=(X_test, y_test): Eğitim sırasında test setiyle de doğrulama kaybını hesaplıyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "predictions_rescaled = scaler.inverse_transform(\n",
    "    np.column_stack((predictions, np.zeros((len(predictions), 3))))\n",
    ")[:, :2]\n",
    "\n",
    "predictions_rescaled[:, 0] = np.clip(predictions_rescaled[:, 0], 0, 100)\n",
    "predictions_rescaled[:, 1] = np.clip(predictions_rescaled[:, 1], 0, 100)\n",
    "\n",
    "\n",
    "y_test_rescaled = scaler.inverse_transform(\n",
    "    np.column_stack((y_test, np.zeros((len(y_test), 3))))\n",
    ")[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.predict(X_test): Test verisi için modelin çıkardığı [SoC, SoH] tahminleri (0–1 aralığında).\n",
    "---------------------------------\n",
    "inverse_transform: Daha önce MinMaxScaler ile ölçeklenen veriyi, orijinal skala (0–100 aralığı) düzeyine geri çevirir.\n",
    "--------------------------------------\n",
    "Neden np.column_stack((predictions, np.zeros((len(predictions), 3)))) yapıyoruz?\n",
    "\n",
    "Orijinal veri 5 sütundan oluşuyordu (SoC, SoH, voltage, current, temperature). Şimdi sadece 2 sütunumuz (SoC, SoH tahmini) var. inverse_transform fonksiyonu, aynı sayıdaki sütuna ihtiyaç duyar. Bu yüzden kalan 3 sütunu “0” olarak ekleyip, inverse_transform sonrasında ilk 2 sütunu alıyoruz.\n",
    "------------------------------------------\n",
    "Tahminlerin 0–100 aralığında kalmasını sağlıyoruz (SoC, SoH fiziksel olarak bu aralığı aşmamalı).\n",
    "-------------------------------------------------\n",
    "Gerçek SoC, SoH değerlerini de aynı şekilde orijinal ölçeğe dönüştürüyoruz ki tahminle karşılaştıralım.\n",
    "\n",
    "3 ve 2 değerlerini neye göre yazdık ?\n",
    "\n",
    "3, geriye kalan 3 sütunun (voltage, current, temperature) yerini sıfırla doldurduğumuzu belirtir.\n",
    "\n",
    "2, inverse_transform sonrası elde edilen (n, 5) matristen sadece ilk 2 sütunu (SoC, SoH) almaya yarar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_rescaled[:, 0], label=\"Actual SoC\", linestyle=\"dashed\")\n",
    "plt.plot(predictions_rescaled[:, 0], label=\"Predicted SoC\", linestyle=\"solid\")\n",
    "...\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
